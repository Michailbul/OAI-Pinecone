{"cells":[{"cell_type":"markdown","id":"8c8fde4c-9222-4265-b72b-8d7693520250","metadata":{},"source":["# Building Chatbots with the OpenAI API and Pinecone"]},{"cell_type":"markdown","id":"a9274661-8d8c-4cc5-901e-5fc497866b89","metadata":{},"source":["## Task 0: Setup"]},{"cell_type":"markdown","id":"2cf847fd-f8f8-49f6-9b43-0eb098239072","metadata":{},"source":["Dependancies setup"]},{"cell_type":"code","execution_count":1,"id":"f96e3639-1515-47ce-9cab-21b2c8a43c64","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":377,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33m  WARNING: The script tqdm is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script pinecone is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script langsmith is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script openai is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/home/repl/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gremlinpython 3.6.1 requires aiohttp<=3.8.1,>=3.8.0, but you have aiohttp 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -qU \\\n","    langchain==0.0.292 \\\n","    openai==0.28.0 \\\n","    datasets==2.10.1 \\\n","    pinecone-client==2.2.4 \\\n","    tiktoken==0.5.1"]},{"cell_type":"markdown","id":"92a9caca-70fd-4ac0-aa15-1bee55c456d3","metadata":{},"source":["# Building a Chatbot"]},{"cell_type":"code","execution_count":3,"id":"f81d6d0e-986b-49f4-94e1-7315a7f0bd67","metadata":{"executionCancelledAt":null,"executionTime":2519,"lastExecutedAt":1705679025222,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chat_models import ChatOpenAI\n\nchat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"},"outputs":[{"ename":"ValidationError","evalue":"1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m----> 3\u001b[0m chat \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/langchain/load/serializable.py:75\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"]}],"source":["from langchain.chat_models import ChatOpenAI\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"]},{"cell_type":"code","execution_count":4,"id":"8f542ae9-c4a0-41aa-a6b6-45585990c246","metadata":{"executionCancelledAt":1705679025181,"executionTime":48,"lastExecutedAt":1698488763511,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.schema import (\n    SystemMessage,\n    HumanMessage,\n    AIMessage\n)\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hi AI, how are you today?\"),\n    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n    HumanMessage(content=\"I'd like to understand string theory.\")\n]"},"outputs":[],"source":["from langchain.schema import (\n","    SystemMessage,\n","    HumanMessage,\n","    AIMessage\n",")\n","\n","messages = [\n","    SystemMessage(content=\"You are a helpful assistant.\"),\n","    HumanMessage(content=\"Hi AI, how are you today?\"),\n","    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n","    HumanMessage(content=\"I'd like to understand string theory.\")\n","]"]},{"cell_type":"code","execution_count":5,"id":"47ad31bf-db2a-444b-a011-26e9336a4e1e","metadata":{"executionCancelledAt":1705679025183,"executionTime":5331,"lastExecutedAt":1698488768843,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"res = chat(messages)\n\nres"},"outputs":[{"data":{"text/plain":["AIMessage(content=\"String theory is a theoretical framework in physics that aims to describe the fundamental structure of the universe. It suggests that elementary particles, such as electrons and quarks, are not point-like particles, but rather tiny, vibrating strings. These strings have different vibrational modes, and each mode corresponds to a different particle with unique properties, such as mass and charge.\\n\\nThe theory incorporates both quantum mechanics and general relativity, attempting to reconcile the two fundamental theories of physics. It suggests that there are more dimensions than the four we commonly experience (three spatial dimensions and one time dimension), possibly up to 10 or even 11 dimensions. These extra dimensions are thought to be compactified or curled up, making them undetectable at our scale.\\n\\nString theory also proposes the existence of additional particles called gravitons, which mediate the force of gravity. This could potentially explain the nature of gravity within the framework of quantum mechanics.\\n\\nIt's important to note that string theory is still a work in progress, and there is no definitive experimental evidence to confirm its predictions. However, it has been influential in advancing our understanding of theoretical physics and has led to many fascinating mathematical developments.\\n\\nIf you have any specific questions about string theory, feel free to ask!\", additional_kwargs={}, example=False)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["res = chat(messages)\n","\n","res"]},{"cell_type":"code","execution_count":6,"id":"38e8a99c-58ea-43ac-8918-64d42541a58d","metadata":{"executionCancelledAt":1705679025184,"executionTime":56,"lastExecutedAt":1698488768900,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(res.content)","outputsMetadata":{"0":{"height":377,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["String theory is a theoretical framework in physics that aims to describe the fundamental structure of the universe. It suggests that elementary particles, such as electrons and quarks, are not point-like particles, but rather tiny, vibrating strings. These strings have different vibrational modes, and each mode corresponds to a different particle with unique properties, such as mass and charge.\n","\n","The theory incorporates both quantum mechanics and general relativity, attempting to reconcile the two fundamental theories of physics. It suggests that there are more dimensions than the four we commonly experience (three spatial dimensions and one time dimension), possibly up to 10 or even 11 dimensions. These extra dimensions are thought to be compactified or curled up, making them undetectable at our scale.\n","\n","String theory also proposes the existence of additional particles called gravitons, which mediate the force of gravity. This could potentially explain the nature of gravity within the framework of quantum mechanics.\n","\n","It's important to note that string theory is still a work in progress, and there is no definitive experimental evidence to confirm its predictions. However, it has been influential in advancing our understanding of theoretical physics and has led to many fascinating mathematical developments.\n","\n","If you have any specific questions about string theory, feel free to ask!\n"]}],"source":["print(res.content)"]},{"cell_type":"markdown","id":"7090e9e9-880d-4b29-909a-a9653474c8b1","metadata":{},"source":["Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."]},{"cell_type":"code","execution_count":7,"id":"51e66e15-dc5d-4f5e-9db2-680f4c90546f","metadata":{"executionCancelledAt":1705679025185,"executionTime":51,"lastExecutedAt":1698488768951,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"messages.append(res)","outputsMetadata":{"0":{"height":505,"type":"stream"}}},"outputs":[],"source":["messages.append(res)"]},{"cell_type":"code","execution_count":8,"id":"be31574f-4801-452e-94a1-a544e50fb4b1","metadata":{"executionCancelledAt":1705679025186,"executionTime":51,"lastExecutedAt":1698488769003,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"prompt = HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\")\n\nmessages.append(prompt)"},"outputs":[],"source":["prompt = HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\")\n","\n","messages.append(prompt)"]},{"cell_type":"code","execution_count":9,"id":"7bc0c721-8eb2-4a20-8491-89a281480ea5","metadata":{"executionCancelledAt":1705679025187,"executionTime":52,"lastExecutedAt":1698488769056,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"messages"},"outputs":[{"data":{"text/plain":["[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}),\n"," HumanMessage(content='Hi AI, how are you today?', additional_kwargs={}, example=False),\n"," AIMessage(content=\"I'm great thank you. How can I help you?\", additional_kwargs={}, example=False),\n"," HumanMessage(content=\"I'd like to understand string theory.\", additional_kwargs={}, example=False),\n"," AIMessage(content=\"String theory is a theoretical framework in physics that aims to describe the fundamental structure of the universe. It suggests that elementary particles, such as electrons and quarks, are not point-like particles, but rather tiny, vibrating strings. These strings have different vibrational modes, and each mode corresponds to a different particle with unique properties, such as mass and charge.\\n\\nThe theory incorporates both quantum mechanics and general relativity, attempting to reconcile the two fundamental theories of physics. It suggests that there are more dimensions than the four we commonly experience (three spatial dimensions and one time dimension), possibly up to 10 or even 11 dimensions. These extra dimensions are thought to be compactified or curled up, making them undetectable at our scale.\\n\\nString theory also proposes the existence of additional particles called gravitons, which mediate the force of gravity. This could potentially explain the nature of gravity within the framework of quantum mechanics.\\n\\nIt's important to note that string theory is still a work in progress, and there is no definitive experimental evidence to confirm its predictions. However, it has been influential in advancing our understanding of theoretical physics and has led to many fascinating mathematical developments.\\n\\nIf you have any specific questions about string theory, feel free to ask!\", additional_kwargs={}, example=False),\n"," HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\", additional_kwargs={}, example=False)]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["messages"]},{"cell_type":"code","execution_count":10,"id":"4f68e1d4-4dcc-4dbe-b7ce-df321f90ae5c","metadata":{"executionCancelledAt":1705679025188,"executionTime":6628,"lastExecutedAt":1698488775684,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"res = chat(messages)\n\nprint(res.content)","outputsMetadata":{"0":{"height":397,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Physicists believe that string theory has the potential to produce a unified theory because it incorporates both quantum mechanics and general relativity, two fundamental theories that have been incredibly successful in explaining different aspects of the physical world but are incompatible with each other.\n","\n","Quantum mechanics describes the behavior of particles at the microscopic level, where the laws of probability and uncertainty play a significant role. On the other hand, general relativity describes the force of gravity and the behavior of massive objects at the cosmic scale, where spacetime is curved.\n","\n","However, when physicists try to combine quantum mechanics with general relativity, they encounter mathematical inconsistencies and infinities. This is known as the problem of quantum gravity. String theory attempts to overcome these issues by replacing point-like particles with tiny vibrating strings. By doing so, it introduces a fundamental length scale and resolves some of the mathematical difficulties that arise in combining quantum mechanics and general relativity.\n","\n","Moreover, string theory has the potential to explain the existence of all the fundamental forces of nature, including gravity, electromagnetism, the strong nuclear force, and the weak nuclear force, within a single framework. This is why it is often referred to as a \"theory of everything\" or a unified theory.\n","\n","While string theory is still a subject of ongoing research and debate, its mathematical elegance and potential to unify the fundamental forces make it an attractive candidate for a unified theory of physics.\n"]}],"source":["res = chat(messages)\n","\n","print(res.content)"]},{"cell_type":"markdown","id":"c536df54-a985-401e-99e7-212004379618","metadata":{},"source":["## Hallucinations solution with RAG"]},{"cell_type":"code","execution_count":12,"id":"c3e22ec5-7a01-46a2-96b9-ee2dda555932","metadata":{"executionCancelledAt":1705679025191,"executionTime":1967,"lastExecutedAt":1698488777699,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"prompt = HumanMessage(content=\"What is so special about Llama 2?\")\nmessages.append(prompt)\n\nres = chat(messages)\nprint(res.content)","outputsMetadata":{"0":{"height":57,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["I'm sorry, but I don't have any information about a specific entity or concept referred to as \"Llama 2.\" It's possible that you might be referring to something that I am not familiar with. Could you please provide more context or clarify your question?\n"]}],"source":["prompt = HumanMessage(content=\"What is so special about Llama 2?\")\n","messages.append(prompt)\n","\n","res = chat(messages)\n","print(res.content)"]},{"cell_type":"markdown","id":"2fdf6188-f9ad-42fd-b254-cfed65434024","metadata":{},"source":["### Instructions\n","\n","Ask GPT about LangChain.\n","\n","- Append the latest AI response to `messages`.\n","- Create a new human message. Assign to `prompt`.\n","    - Use the content `\"Can you tell me about the LLMChain in LangChain?\"`.\n","- Append the prompt to `messages`.\n","- Send the messages to GPT. Assign to `res`.\n","- Print the contents of the response."]},{"cell_type":"code","execution_count":13,"id":"13e182b4-3b48-4d28-88dd-27abc9ad4a1c","metadata":{"executionCancelledAt":1705679025192,"executionTime":1962,"lastExecutedAt":1698488779661,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"messages.append(res)\nprompt = HumanMessage(content=\"Can you tell me about the LLMChain in LangChain?\")\nmessages.append(prompt)\n\nres = chat(messages)\nprint(res.content)","outputsMetadata":{"0":{"height":77,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["I apologize, but I'm not familiar with the specific terms \"LLMChain\" or \"LangChain.\" It's possible that these terms are related to a particular project, technology, or concept that I am not aware of. Without more context or information, I won't be able to provide you with a detailed explanation. Could you please provide more details or clarify your question further?\n"]}],"source":["messages.append(res)\n","prompt = HumanMessage(content=\"Can you tell me about the LLMChain in LangChain?\")\n","messages.append(prompt)\n","\n","res = chat(messages)\n","print(res.content)"]},{"cell_type":"markdown","id":"ef46c06c-d226-49ce-b467-78408945ff4f","metadata":{},"source":["There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."]},{"cell_type":"markdown","id":"9e8fb83d-8c94-4d11-b21a-12c455a7f873","metadata":{},"source":["### Instructions\n","\n","Create a string of knowledge about chains.\n","\n","- *Read the descriptions of LLMChains, Chains, and LangChain given in `llmchain_information`.*\n","- Combine the list of description strings into a single string. Assign to `source_knowledge`."]},{"cell_type":"code","execution_count":14,"id":"711778a5-76d5-40d2-8ed1-2b12a89a474d","metadata":{"executionCancelledAt":1705679025193,"executionTime":55,"lastExecutedAt":1698488779716,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# A description of LLMChains, Chains, and LangChain \nllmchain_information = [\n    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n]\nlen(llmchain_information)"},"outputs":[{"data":{"text/plain":["3"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# A description of LLMChains, Chains, and LangChain \n","llmchain_information = [\n","    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n","    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n","    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n","]\n","len(llmchain_information)"]},{"cell_type":"code","execution_count":15,"id":"f63762f2-0cb2-48ee-8d51-8fc6b3165fcd","metadata":{"executionCancelledAt":1705679025194,"executionTime":56,"lastExecutedAt":1698488779772,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"source_knowledge = \"\\n\".join(llmchain_information)\nsource_knowledge"},"outputs":[{"data":{"text/plain":["'A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\\nChains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["source_knowledge = \"\\n\".join(llmchain_information)\n","source_knowledge"]},{"cell_type":"code","execution_count":16,"id":"58a71fbf-6b81-42f5-8073-ddfd3139e351","metadata":{"executionCancelledAt":1705679025195,"executionTime":47,"lastExecutedAt":1698488779819,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"query = \"Can you tell me about the LLMChain in LangChain?\"\n\naugmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\nthe contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\nContexts:\n{source_knowledge}\n\nQuery: {query}\"\"\""},"outputs":[],"source":["query = \"Can you tell me about the LLMChain in LangChain?\"\n","\n","augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\n","the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n","\n","Contexts:\n","{source_knowledge}\n","\n","Query: {query}\"\"\""]},{"cell_type":"code","execution_count":17,"id":"99e5faa0-e6dd-43a1-ba33-96fdeea31f9e","metadata":{"executionCancelledAt":1705679025197,"executionTime":52,"lastExecutedAt":1698488779872,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(augmented_prompt)","outputsMetadata":{"0":{"height":317,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Using the contexts below, answer the query. If some information is not provided within\n","the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n","\n","Contexts:\n","A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\n","Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\n","LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n","\n","Query: Can you tell me about the LLMChain in LangChain?\n"]}],"source":["print(augmented_prompt)"]},{"cell_type":"code","execution_count":18,"id":"a7e0ee47-69f2-4f21-b48d-279e9ae5df3e","metadata":{"executionCancelledAt":1705679025198,"executionTime":48,"lastExecutedAt":1698488779920,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(messages[-1])","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["content='Can you tell me about the LLMChain in LangChain?' additional_kwargs={} example=False\n"]}],"source":["print(messages[-1])"]},{"cell_type":"code","execution_count":19,"id":"901893e6-726d-4a42-805f-ee2d24b3b8a2","metadata":{"executionCancelledAt":1705679025200,"executionTime":51,"lastExecutedAt":1698488779971,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"messages[-1] = HumanMessage(content=augmented_prompt)","outputsMetadata":{"0":{"height":368,"type":"stream"}}},"outputs":[],"source":["messages[-1] = HumanMessage(content=augmented_prompt)"]},{"cell_type":"code","execution_count":20,"id":"ebbf365d-0215-4f6f-bb5b-146323b559b4","metadata":{"executionCancelledAt":1705679025201,"executionTime":4842,"lastExecutedAt":1698488784813,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"res = chat(messages)\nprint(res.content)","outputsMetadata":{"0":{"height":217,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Based on the provided context, it appears that the LLMChain is a type of chain within the LangChain framework for developing applications powered by language models. The LLMChain consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser.\n","\n","The purpose of the LLMChain is to take multiple input variables, format them into a prompt using the PromptTemplate, and then pass that prompt to the language model (LLM or ChatModel). Finally, if an OutputParser is provided, it is used to parse the output of the language model into a final format.\n","\n","In summary, the LLMChain is a component of the LangChain framework that facilitates the integration of language models into applications by providing a structure for handling input variables, formatting prompts, and parsing the model's output.\n"]}],"source":["res = chat(messages)\n","print(res.content)"]},{"cell_type":"markdown","id":"97fa3cfe-9223-4593-85fd-6c21f788d46e","metadata":{},"source":["##  Importing the Data"]},{"cell_type":"code","execution_count":21,"id":"16f97002-b358-4a33-bb50-0401feda259b","metadata":{"executionCancelledAt":1705679025201,"executionTime":1388,"lastExecutedAt":1698488786201,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from datasets import load_dataset\n\ndata = load_dataset(\"jamescalam/llama-2-arxiv-papers-chunked\", split=\"train\")\ndata","outputsMetadata":{"1":{"height":77,"type":"stream"},"6":{"height":77,"type":"stream"}}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e30f7df755154538834f2c14a0b67ffb","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/409 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset json/jamescalam--llama-2-arxiv-papers-chunked to /home/repl/.cache/huggingface/datasets/jamescalam___json/jamescalam--llama-2-arxiv-papers-chunked-ea255a807f3039a6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5761513562b3444eb5758bc1c7260888","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19d5687bb51b4bcdaab590c4efab4685","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a137673328214c25a78d6804d3ca86b0","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de4cafa19cb349c9be2b5f61434f9e1a","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset json downloaded and prepared to /home/repl/.cache/huggingface/datasets/jamescalam___json/jamescalam--llama-2-arxiv-papers-chunked-ea255a807f3039a6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n","    num_rows: 4838\n","})"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","\n","data = load_dataset(\"jamescalam/llama-2-arxiv-papers-chunked\", split=\"train\")\n","data"]},{"cell_type":"markdown","id":"796c12aa-e78a-4e62-9205-73cc6f3908cd","metadata":{},"source":["### Instructions\n","\n","Print a record of dataset to get a feel for what they contain."]},{"cell_type":"code","execution_count":22,"id":"0e4c8dfd-2c65-4d2f-8ae1-d356daf0a3d8","metadata":{"executionCancelledAt":1705679025202,"executionTime":383,"lastExecutedAt":1698488786584,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"data[0]"},"outputs":[{"data":{"text/plain":["{'doi': '1102.0183',\n"," 'chunk-id': '0',\n"," 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but',\n"," 'id': '1102.0183',\n"," 'title': 'High-Performance Neural Networks for Visual Object Classification',\n"," 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n"," 'source': 'http://arxiv.org/pdf/1102.0183',\n"," 'authors': ['Dan C. Cireşan',\n","  'Ueli Meier',\n","  'Jonathan Masci',\n","  'Luca M. Gambardella',\n","  'Jürgen Schmidhuber'],\n"," 'categories': ['cs.AI', 'cs.NE'],\n"," 'comment': '12 pages, 2 figures, 5 tables',\n"," 'journal_ref': None,\n"," 'primary_category': 'cs.AI',\n"," 'published': '20110201',\n"," 'updated': '20110201',\n"," 'references': []}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["data[0]"]},{"cell_type":"markdown","id":"7af0beaa-68f8-4cc3-831f-1118e79f45b3","metadata":{},"source":["## Building the Knowledge Base"]},{"cell_type":"markdown","id":"92558679-3d66-419d-b30d-783456b992e5","metadata":{},"source":["We now have a dataset that can serve as our chatbot knowledge base. The next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database."]},{"cell_type":"code","execution_count":23,"id":"0be5dcfe-1fd1-432c-9c95-77572f27e2fe","metadata":{"executionCancelledAt":1705679025203,"executionTime":659,"lastExecutedAt":1698488788386,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import os\nimport pinecone\n\npinecone.init(\n    api_key=os.environ[\"PINECONE_API_KEY\"],\n    environment=os.environ[\"PINECONE_ENVIRONMENT\"]\n)"},"outputs":[],"source":["import os\n","import pinecone\n","\n","pinecone.init(\n","    api_key=os.environ[\"PINECONE_API_KEY\"],\n","    environment=os.environ[\"PINECONE_ENVIRONMENT\"]\n",")"]},{"cell_type":"code","execution_count":24,"id":"a9c31353-0b6c-40e4-8b13-582b39004f55","metadata":{"executionCancelledAt":1705679025204,"executionTime":33401,"lastExecutedAt":1698489011979,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import time\n\nindex_name = \"llama-2-rag\"\n\nif index_name not in pinecone.list_indexes():\n    pinecone.create_index(\n        index_name, dimension=1536, metric=\"cosine\"\n    )\n    while not pinecone.describe_index(index_name).status[\"ready\"]:\n        time.sleep(1)\n        \nindex = pinecone.Index(index_name)"},"outputs":[],"source":["import time\n","\n","index_name = \"llama-2-rag\"\n","\n","if index_name not in pinecone.list_indexes():\n","    pinecone.create_index(\n","        index_name, dimension=1536, metric=\"cosine\"\n","    )\n","    while not pinecone.describe_index(index_name).status[\"ready\"]:\n","        time.sleep(1)\n","        \n","index = pinecone.Index(index_name)"]},{"cell_type":"code","execution_count":25,"id":"ea941ab8-99ab-4600-b035-7a892b0bd88d","metadata":{"executionCancelledAt":1705679025206,"executionTime":232,"lastExecutedAt":1698489028339,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"index.describe_index_stats()"},"outputs":[{"data":{"text/plain":["{'dimension': 1536,\n"," 'index_fullness': 0.0,\n"," 'namespaces': {},\n"," 'total_vector_count': 0}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["index.describe_index_stats()"]},{"cell_type":"code","execution_count":26,"id":"7fc53547-8e89-4700-9a69-95bca1bedd30","metadata":{"executionCancelledAt":1705679025207,"executionTime":49,"lastExecutedAt":1698489100619,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.embeddings.openai import OpenAIEmbeddings\n\nembed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"},"outputs":[],"source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","\n","embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"]},{"cell_type":"code","execution_count":27,"id":"5b86b13a-591f-4b8e-b3cc-33c24c076d5b","metadata":{"executionCancelledAt":1705679025209,"executionTime":1272,"lastExecutedAt":1698489196826,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"texts = [\n    \"this is a sentence\",\n    \"this is another sentence\"\n]\n\nres = embed_model.embed_documents(texts=texts)\nlen(res), len(res[0])"},"outputs":[{"data":{"text/plain":["(2, 1536)"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["texts = [\n","    \"this is a sentence\",\n","    \"this is another sentence\"\n","]\n","\n","res = embed_model.embed_documents(texts=texts)\n","len(res), len(res[0])"]},{"cell_type":"code","execution_count":31,"id":"7f35bdc2-c8b2-458b-a5b8-636115c50bec","metadata":{"executionCancelledAt":1705679025210,"executionTime":51,"lastExecutedAt":1698489533412,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"data[0][\"doi\"], data[1][\"doi\"], data[0][\"chunk-id\"], data[1][\"chunk-id\"]"},"outputs":[{"data":{"text/plain":["('1102.0183', '1102.0183', '0', '1')"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["data[0][\"doi\"], data[1][\"doi\"], data[0][\"chunk-id\"], data[1][\"chunk-id\"]"]},{"cell_type":"code","execution_count":32,"id":"1a9e78d0-8e0e-4503-b97f-713c2e475565","metadata":{"executionCancelledAt":1705679025211,"executionTime":12,"lastExecutedAt":1698489659921,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"data"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n","    num_rows: 4838\n","})"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"code","execution_count":33,"id":"ae4bf4ac-16ff-4c70-baef-dcd2f746a200","metadata":{"executionCancelledAt":1705679025213,"executionTime":74075,"lastExecutedAt":1698489896718,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from tqdm import tqdm\n\ndata = data.to_pandas()\n\nbatch_size = 100\n\nfor i in tqdm(range(0, len(data), batch_size)):\n    i_end = min(i+batch_size, len(data))\n    batch = data.iloc[i:i_end]\n    ids = [f\"{x['doi']}-{x['chunk-id']}\" for _, x in batch.iterrows()]\n    texts = [x[\"chunk\"] for _, x in batch.iterrows()]\n    embeds = embed_model.embed_documents(texts)\n    metadata = [\n        {\"text\": x[\"chunk\"],\n         \"title\": x[\"title\"],\n         \"source\": x[\"source\"]} for _, x in  batch.iterrows()\n    ]\n    # [(id1, embed1, metadata1), (id2, embed2, metadata2), ...]\n    index.upsert(vectors=zip(ids, embeds, metadata))","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 49/49 [01:14<00:00,  1.51s/it]\n"]}],"source":["from tqdm import tqdm\n","\n","data = data.to_pandas()\n","\n","batch_size = 100\n","\n","for i in tqdm(range(0, len(data), batch_size)):\n","    i_end = min(i+batch_size, len(data))\n","    batch = data.iloc[i:i_end]\n","    ids = [f\"{x['doi']}-{x['chunk-id']}\" for _, x in batch.iterrows()]\n","    texts = [x[\"chunk\"] for _, x in batch.iterrows()]\n","    embeds = embed_model.embed_documents(texts)\n","    metadata = [\n","        {\"text\": x[\"chunk\"],\n","         \"title\": x[\"title\"],\n","         \"source\": x[\"source\"]} for _, x in  batch.iterrows()\n","    ]\n","    # [(id1, embed1, metadata1), (id2, embed2, metadata2), ...]\n","    index.upsert(vectors=zip(ids, embeds, metadata))"]},{"cell_type":"code","execution_count":34,"id":"48aa5c57-c923-49fc-b1dd-d60f6fa26d64","metadata":{"executionCancelledAt":1705679025214,"executionTime":75,"lastExecutedAt":1698489901904,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"index.describe_index_stats()"},"outputs":[{"data":{"text/plain":["{'dimension': 1536,\n"," 'index_fullness': 0.0,\n"," 'namespaces': {'': {'vector_count': 4838}},\n"," 'total_vector_count': 4838}"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["index.describe_index_stats()"]},{"cell_type":"markdown","id":"6b90e6fc-f627-40a2-8276-7df46b5d7155","metadata":{},"source":["## RAG"]},{"cell_type":"code","execution_count":35,"id":"d91af8d9-9e0b-4246-ba92-08769b7f6017","metadata":{"executionCancelledAt":1705679025215,"executionTime":50,"lastExecutedAt":1698489995687,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.vectorstores import Pinecone\n\ntext_field = \"text\"\n\nvectorstore = Pinecone(\n    index, embed_model.embed_query, text_field\n)"},"outputs":[],"source":["from langchain.vectorstores import Pinecone\n","\n","text_field = \"text\"\n","\n","vectorstore = Pinecone(\n","    index, embed_model.embed_query, text_field\n",")"]},{"cell_type":"code","execution_count":36,"id":"11343325-0736-4b08-b85b-1d4735a0402d","metadata":{"executionCancelledAt":1705679025216,"executionTime":304,"lastExecutedAt":1698490032321,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"query = \"What is so special about Llama 2?\"\n\nvectorstore.similarity_search(query, k=3)"},"outputs":[{"data":{"text/plain":["[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n"," Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n"," Document(page_content='Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["query = \"What is so special about Llama 2?\"\n","\n","vectorstore.similarity_search(query, k=3)"]},{"cell_type":"code","execution_count":40,"id":"19b86b04-9630-4fb8-a4f9-9f5b38300623","metadata":{"executionCancelledAt":1705679025217,"executionTime":10,"lastExecutedAt":1698490248062,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def augment_prompt(query: str):\n    results = vectorstore.similarity_search(query, k=3)\n    source_knowledge = \"\\n\".join([x.page_content for x in results])\n    augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\nthe contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\nContexts:\n{source_knowledge}\n\nQuery: {query}\"\"\"\n    return augmented_prompt"},"outputs":[],"source":["def augment_prompt(query: str):\n","    results = vectorstore.similarity_search(query, k=3)\n","    source_knowledge = \"\\n\".join([x.page_content for x in results])\n","    augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\n","the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n","\n","Contexts:\n","{source_knowledge}\n","\n","Query: {query}\"\"\"\n","    return augmented_prompt"]},{"cell_type":"markdown","id":"b4a4f1a7-5d0b-4e7f-ab38-fbada51ba1d8","metadata":{},"source":["Using this we produce an augmented prompt:"]},{"cell_type":"code","execution_count":42,"id":"0ba045f5-88d7-4ce3-9ae6-e96cafa96e02","metadata":{"executionCancelledAt":1705679025218,"executionTime":302,"lastExecutedAt":1698490260911,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(augment_prompt(query))","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Using the contexts below, answer the query. If some information is not provided within\n","the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n","\n","Contexts:\n","Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n","Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n","Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n","Sergey Edunov Thomas Scialom\u0003\n","GenAI, Meta\n","Abstract\n","In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n","large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n","Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n","models outperform open-source chat models on most benchmarks we tested, and based on\n","ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n","asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n","preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\n","computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n","the community to advance AI alignment research.\n","In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n","L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n","L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n","be on par with some of the closed-source models, at least on the human evaluations we performed (see\n","Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\n","Grave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\n","arXiv:2302.13971 , 2023.\n","Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n","and Illia Polosukhin. Attention is all you need, 2017.\n","Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\n","David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\n","multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n","Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint\n","\n","Query: What is so special about Llama 2?\n"]}],"source":["print(augment_prompt(query))"]},{"cell_type":"markdown","id":"5be7c70d-a1c2-414b-b09b-afbc046503df","metadata":{},"source":["There is still a lot of text here, so let's pass it onto our chat model to see how it performs."]},{"cell_type":"code","execution_count":43,"id":"6f70d2a0-d5af-448b-8f00-0aec873ac250","metadata":{"executionCancelledAt":1705679025219,"executionTime":5775,"lastExecutedAt":1698490333721,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"prompt = HumanMessage(content=augment_prompt(query))\n\nmessages.append(prompt)\nres = chat(messages)\nprint(res.content)","outputsMetadata":{"0":{"height":257,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Based on the provided context, it is mentioned that Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging from 7 billion to 70 billion parameters. These LLMs, such as L/l.sc/a.sc/m.sc/a.sc/t.sc and L/l.sc/a.sc/m.sc/a.sc/t.sc-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases. The models have been evaluated and found to outperform open-source chat models on most benchmarks tested, and they are considered as potential substitutes for closed-source models in terms of helpfulness and safety.\n","\n","Furthermore, the authors describe their approach to fine-tuning and safety in detail, highlighting that closed-source LLMs are heavily fine-tuned to align with human preferences, enhancing their usability and safety. The development and release of Llama 2 aims to provide an open and efficient foundation for language models, allowing for advancements in AI alignment research.\n","\n","However, it's important to note that the context provided is limited, and there may be additional information about Llama 2 that is not included.\n"]}],"source":["prompt = HumanMessage(content=augment_prompt(query))\n","\n","messages.append(prompt)\n","res = chat(messages)\n","print(res.content)"]},{"cell_type":"markdown","id":"f3547c02-31ea-4847-8042-7095d22a0fe9","metadata":{},"source":["We can continue with more Llama 2 questions. Let's try _without_ RAG first:"]},{"cell_type":"code","execution_count":44,"id":"00e797e1-5526-416b-be41-88925fa14960","metadata":{"executionCancelledAt":1705679025220,"executionTime":4663,"lastExecutedAt":1698490475739,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"prompt = HumanMessage(\n    content=\"What safety measures were used in the development of llama 2?\"\n)\n\nres = chat(messages + [prompt])\nprint(res.content)","outputsMetadata":{"0":{"height":177,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Based on the provided context, the safety measures used in the development of Llama 2 are mentioned but not explicitly described. The text mentions that the fine-tuned LLMs in Llama 2, such as L/l.sc/a.sc/m.sc/a.sc/t.sc and L/l.sc/a.sc/m.sc/a.sc/t.sc-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases and appear to be on par with some closed-source models in terms of safety. It also mentions that closed-source models are heavily fine-tuned to align with human preferences, enhancing their usability and safety.\n","\n","However, the specific safety measures employed in the development of Llama 2 are not elaborated upon in the given information. It is possible that the paper or source from which this information is derived may provide more details on the safety measures utilized.\n"]}],"source":["prompt = HumanMessage(\n","    content=\"What safety measures were used in the development of llama 2?\"\n",")\n","\n","res = chat(messages + [prompt])\n","print(res.content)"]},{"cell_type":"code","execution_count":46,"id":"412a5fb9-5bbb-46cd-8636-6bcb7f575605","metadata":{"executionCancelledAt":1705679025221,"executionTime":2855,"lastExecutedAt":1698490604812,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"prompt = HumanMessage(\n    content=augment_prompt(\"What safety measures were used in the development of llama 2?\")\n)\n\nres = chat(messages + [prompt])\nprint(res.content)","outputsMetadata":{"0":{"height":117,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["According to the provided contexts, the safety measures used in the development of Llama 2 include safety-specific data annotation and tuning, red-teaming, iterative evaluations, and a thorough approach to improving the safety of the fine-tuned large language models (LLMs). These measures were taken to increase the safety of the models and ensure responsible development of LLMs. The paper also mentions that a detailed description of their fine-tuning methodology and safety approach is provided, which can further shed light on the specific safety measures implemented.\n"]}],"source":["prompt = HumanMessage(\n","    content=augment_prompt(\"What safety measures were used in the development of llama 2?\")\n",")\n","\n","res = chat(messages + [prompt])\n","print(res.content)"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
